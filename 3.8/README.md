# Example setup that uses Kubernetes

I deployed an example / POC install of Kubernetes and deployed some pods from the example exercise on it. 

Because this is DevOps and I needed some practice, I ansiblated the install. The playbook runs only on CentOS 7. 


## Deploying the cluster

First the install hosts needs to be set to the hosts file found in inventories/hosts: 

```
[masters]
master1 ansible_host=10.10.10.10
[workers]
node1 ansible_host=10.10.10.11
node2 ansible_host=10.10.10.12
```

1 + 2 is just a minimal configuration and this could be run on any number of nodes. 

The install was initiated with a command (CIDR netmask of the cluster is given as an extra variable): 

```
$ ansible-playbook -i inventories/hosts  -u kube -e cidr_mask=10.244.0.0/16 run-install-cluster.yml
```

run-install-cluster.yml will first run these playbooks an all nodes: 


- preinstall-check.yml
- prepare-environment.yml
- install-docker-ce.yml
- install-kubernetes.yml

After that it will configure the cluster, configuring master server(s) separately and all worker found in the hosts file. I'm not going in to details as the playbook documents itself. 

After the install all nodes shows "Ready"

```
[kube@master1 ~]$ kubectl get nodes
NAME                 STATUS   ROLES    AGE   VERSION
master1.liitos.biz   Ready    master   58s   v1.17.1
node1.liitos.biz     Ready    <none>   36s   v1.17.1
node2.liitos.biz     Ready    <none>   33s   v1.17.1
```

## Deploying the services

First create a deployment files for the frontend and backend services (images are pulled from Dockerhub): 

```
[kube@master1 ~]$Â cat <<EOF >> frontend-deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend-deployment
  labels:
    app: frontend
spec:
  replicas: 1
  selector:
    matchLabels:
      app: frontend
  template:
    metadata:
      labels:
        app: frontend
    spec:
      containers:
      - name: frontend
        image: liitos/example_project:frontend 
        env:
        - name: APIPATH
          value: "/api/books"
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 5000

EOF
```
```
[kube@master1 ~]$ cat <<EOF >> backend-deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend-deployment
  labels:
    app: backend
spec:
  replicas: 1
  selector:
    matchLabels:
      app: backend
  template:
    metadata:
      labels:
        app: backend
    spec:
      containers:
      - name: backend
        image: liitos/example_project:backend 
        env:
        - name: APIPATH
          value: "/api/books"
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 8000
EOF
```


Create the actual services: 

```
kube@master1 ~]$ kubectl create -f frontend-deployment.yml 
deployment.apps/frontend-deployment created

[kube@master1 ~]$ kubectl create -f backend-deployment.yml 
deployment.apps/backend-deployment created 
```

Let's verify: 

```
[kube@master1 ~]$ kubectl get deployment
NAME                  READY   UP-TO-DATE   AVAILABLE   AGE
backend-deployment    1/1     1            1           1m
frontend-deployment   1/1     1            1           1m
```
```
[kube@master1 ~]$ kubectl get pods -o wide
NAME                                   READY   STATUS    RESTARTS   AGE   IP            NODE               NOMINATED NODE   READINESS GATES
backend-deployment-5c9df6fd68-pg26h    1/1     Running   0          1m   10.244.2.14   node2.liitos.biz   <none>           <none>
frontend-deployment-57f85975cf-km7xl   1/1     Running   0          1m   10.244.1.17   node1.liitos.biz   <none>           <none>
```

Now when we want to expose the services in a public IP address, well need to forward some ports: 

```
[kube@master1 ~]$ kubectl port-forward -n default frontend-deployment-57f85975cf-stxl7 --address 94.237.25.222 5000:5000
Forwarding from 94.237.25.222:5000 -> 5000
[1] 23295
```

```
[kube@master1 ~]$ kubectl port-forward -n default backend-deployment-7b97c4dcbb-qztzc --address 94.237.25.222 8000:8000
Forwarding from 94.237.25.222:8000 -> 8000
[2] 23634
```

Verify again: 
```
[kube@master1 ~]$ curl -I 94.237.25.222:5000
Handling connection for 5000
HTTP/1.1 200 OK
Last-Modified: Fri, 17 Jan 2020 13:17:18 GMT
Content-Length: 454
Content-Disposition: inline; filename="index.html"
Accept-Ranges: bytes
Content-Type: text/html; charset=utf-8
Vary: Accept-Encoding
Date: Fri, 17 Jan 2020 13:32:05 GMT
Connection: keep-alive
```
```
[kube@master1 ~]$ curl -I 94.237.25.222:8000
Handling connection for 8000
HTTP/1.1 200 OK
X-Powered-By: Express
Access-Control-Allow-Origin: http://node1.liitos.biz:5000
Access-Control-Allow-Headers: Origin, X-Requested-With, Content-Type, Accept
Content-Type: text/html; charset=utf-8
Content-Length: 56
ETag: W/"38-kNCveM+0J9cBkxUgznUtzYVrhXE"
Date: Fri, 17 Jan 2020 13:32:04 GMT
Connection: keep-alive
```
As the local firewall is closed, we need to open it to expose the service to public network: 

```
[kube@master1 ~]# sudo firewall-cmd --zone=public --permanent --add-port=5000/tcp
success
[kube@master1 ~]# sudo firewall-cmd --zone=public --permanent --add-port=8000/tcp
success
[kube@master1 ~]# sudo firewall-cmd --reload
success
```

Now the service is usable from URL http://master1.liitos.biz (atleast as long I run out of VPS credits)

The port forwarding is absolutely an ugly way to expose the pods. I should've done separate services type of Load Balancer to NodePort to enable (automatic) replication of pods and doing thing "Kubernetes" way. Unfortunately I spent too much time on Ansible, so I ran of time to do this as I would've wanted to. 


